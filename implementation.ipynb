{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljy/miniconda2/envs/xgb/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN配置参数\"\"\"\n",
    "    embedding_size = 64  # 词向量维度\n",
    "    seq_length = 600  # 序列长度\n",
    "    num_classes = 10  # 类别数\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺寸\n",
    "    vocab_size = 5000  # 词汇表大小\n",
    "\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 10  # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 100  # 每多少轮输出一次结果\n",
    "    save_per_batch = 10  # 每多少轮存入tensorboard\n",
    "\n",
    "    l2_reg_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现思路：\n",
    "# 将对x的标签化放在模型中完成，只将对y的label化放在外面。个人倾向于都放在模型中，后续进行修改\n",
    "# embedding - cnn - maxpooling - fc - dropout - classify - result\n",
    "class TextCNN(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        # placrholders for three inputs\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # init cnn model different the original form, i prefer this one\n",
    "        self.cnn()\n",
    "    def cnn(self, ):\n",
    "        # embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            # w is the embedding matrix thar we learn during training and use randomuniform distribution to initialize it\n",
    "            W = tf.Variable(tf.random_uniform([self.config.vocab_size, self.config.embedding_size], -1, 1), name = \"embedding_W\")\n",
    "            embedded_ouput = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            print(embedded_ouput)\n",
    "            \n",
    "        # define the cnn layer with max pooling\n",
    "        # use 2d input instead of 4d so we could jsut conv1d, simplify the calculation of dimentsions\n",
    "        with tf.name_scope('cnn'):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedded_ouput, self.config.num_filters, self.config.kernel_size, name='cnn_conv')\n",
    "            # global max pooling layer\n",
    "            # just get the max one on dimension 1\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        # add a fully-connect with a dropout\n",
    "        \n",
    "        with tf.name_scope(\"dropout\", ):\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "            \n",
    "        with tf.name_scope(\"score\", ):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[self.config.hidden_dim, self.config.num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.config.num_classes]), name=\"b\")\n",
    "            self.l2_loss += tf.nn.l2_loss(W)\n",
    "            self.l2_loss += tf.nn.l2_loss(b)\n",
    "            \n",
    "            self.logits = tf.nn.xw_plus_b(fc, W, b, name=\"scores\")\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别\n",
    "        \n",
    "        with tf.name_scope(\"loss\",):\n",
    "            # loss function cross_entropy\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy) + self.config.l2_reg_lambda * self.l2_loss\n",
    "            \n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\", ):\n",
    "            # 准确率\n",
    "            correct_predictions = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name = \"accuracy\")\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CNN model...\n",
      "Tensor(\"embedding/embedding_lookup:0\", shape=(?, 600, 64), dtype=float32, device=/device:CPU:0)\n",
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "Time usage: 0:00:13\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Iter:      0, Train Loss:    3.7, Train Acc:  10.94%, Val Loss:    3.7, Val Acc:   9.34%, Time: 0:00:06 *\n",
      "Iter:    100, Train Loss:    2.1, Train Acc:  64.06%, Val Loss:    2.3, Val Acc:  49.60%, Time: 0:00:58 *\n",
      "Iter:    200, Train Loss:    0.9, Train Acc:  85.94%, Val Loss:    1.2, Val Acc:  73.74%, Time: 0:01:49 *\n",
      "Iter:    300, Train Loss:   0.69, Train Acc:  81.25%, Val Loss:   0.93, Val Acc:  76.42%, Time: 0:02:40 *\n",
      "Iter:    400, Train Loss:   0.53, Train Acc:  92.19%, Val Loss:   0.74, Val Acc:  81.94%, Time: 0:03:32 *\n",
      "Iter:    500, Train Loss:   0.37, Train Acc:  96.88%, Val Loss:   0.63, Val Acc:  86.04%, Time: 0:04:23 *\n",
      "Iter:    600, Train Loss:   0.28, Train Acc:  95.31%, Val Loss:   0.54, Val Acc:  87.64%, Time: 0:05:15 *\n",
      "Iter:    700, Train Loss:   0.39, Train Acc:  92.19%, Val Loss:   0.52, Val Acc:  88.02%, Time: 0:06:07 *\n",
      "Epoch: 2\n",
      "Iter:    800, Train Loss:   0.27, Train Acc:  95.31%, Val Loss:   0.54, Val Acc:  86.28%, Time: 0:06:58 \n",
      "Iter:    900, Train Loss:   0.22, Train Acc:  95.31%, Val Loss:   0.44, Val Acc:  91.12%, Time: 0:07:58 *\n",
      "Iter:   1000, Train Loss:   0.19, Train Acc:  96.88%, Val Loss:   0.42, Val Acc:  90.28%, Time: 0:08:50 \n",
      "Iter:   1100, Train Loss:    0.2, Train Acc:  96.88%, Val Loss:   0.41, Val Acc:  90.46%, Time: 0:09:42 \n",
      "Iter:   1200, Train Loss:   0.24, Train Acc:  95.31%, Val Loss:    0.4, Val Acc:  90.82%, Time: 0:10:34 \n",
      "Iter:   1300, Train Loss:   0.23, Train Acc:  95.31%, Val Loss:   0.36, Val Acc:  91.74%, Time: 0:11:26 *\n",
      "Iter:   1400, Train Loss:   0.19, Train Acc:  95.31%, Val Loss:   0.39, Val Acc:  90.88%, Time: 0:12:18 \n",
      "Iter:   1500, Train Loss:   0.22, Train Acc:  93.75%, Val Loss:   0.36, Val Acc:  90.82%, Time: 0:13:13 \n",
      "Epoch: 3\n",
      "Iter:   1600, Train Loss:   0.12, Train Acc: 100.00%, Val Loss:   0.35, Val Acc:  91.12%, Time: 0:14:08 \n",
      "Iter:   1700, Train Loss:   0.18, Train Acc:  96.88%, Val Loss:   0.35, Val Acc:  91.30%, Time: 0:15:04 \n",
      "Iter:   1800, Train Loss:   0.16, Train Acc:  96.88%, Val Loss:   0.32, Val Acc:  91.98%, Time: 0:16:08 *\n",
      "Iter:   1900, Train Loss:  0.098, Train Acc:  98.44%, Val Loss:   0.32, Val Acc:  92.26%, Time: 0:17:07 *\n",
      "Iter:   2000, Train Loss:   0.22, Train Acc:  93.75%, Val Loss:   0.31, Val Acc:  92.46%, Time: 0:18:02 *\n",
      "Iter:   2100, Train Loss:   0.34, Train Acc:  90.62%, Val Loss:    0.3, Val Acc:  93.26%, Time: 0:19:04 *\n",
      "Iter:   2200, Train Loss:   0.16, Train Acc:  96.88%, Val Loss:   0.28, Val Acc:  93.38%, Time: 0:20:01 *\n",
      "Iter:   2300, Train Loss:   0.15, Train Acc:  95.31%, Val Loss:    0.3, Val Acc:  92.74%, Time: 0:21:04 \n",
      "Epoch: 4\n",
      "Iter:   2400, Train Loss:  0.093, Train Acc: 100.00%, Val Loss:   0.32, Val Acc:  92.92%, Time: 0:22:01 \n",
      "Iter:   2500, Train Loss:   0.14, Train Acc:  96.88%, Val Loss:   0.31, Val Acc:  93.12%, Time: 0:22:59 \n",
      "Iter:   2600, Train Loss:  0.071, Train Acc: 100.00%, Val Loss:   0.31, Val Acc:  92.88%, Time: 0:23:58 \n",
      "Iter:   2700, Train Loss:  0.097, Train Acc:  96.88%, Val Loss:   0.29, Val Acc:  93.14%, Time: 0:24:50 \n",
      "Iter:   2800, Train Loss:   0.13, Train Acc:  96.88%, Val Loss:    0.3, Val Acc:  92.40%, Time: 0:25:43 \n",
      "Iter:   2900, Train Loss:   0.12, Train Acc:  98.44%, Val Loss:   0.31, Val Acc:  91.72%, Time: 0:26:36 \n",
      "Iter:   3000, Train Loss:   0.19, Train Acc:  93.75%, Val Loss:   0.29, Val Acc:  92.36%, Time: 0:27:28 \n",
      "Iter:   3100, Train Loss:   0.17, Train Acc:  95.31%, Val Loss:   0.28, Val Acc:  92.30%, Time: 0:28:23 \n",
      "Epoch: 5\n",
      "Iter:   3200, Train Loss:  0.079, Train Acc: 100.00%, Val Loss:   0.29, Val Acc:  92.80%, Time: 0:29:22 \n",
      "No optimization for a long time, auto-stopping...\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # 添加这句的目的是在jupyter notebook里面清楚缓存，不然会出现变量重用的问题\n",
    "print('Configuring CNN model...')\n",
    "config = TCNNConfig()\n",
    "if not os.path.exists(vocab_dir):  # 如果不存在词汇表，重建\n",
    "    build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "categories, cat_to_id = read_category()\n",
    "words, word_to_id = read_vocab(vocab_dir)\n",
    "config.vocab_size = len(words)\n",
    "model = TextCNN(config)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/textcnn/best_validation\n",
      "Testing...\n",
      "Test Loss:    0.2, Test Acc:  95.63%\n",
      "Precision, Recall and F1-Score...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         体育       0.99      0.99      0.99      1000\n",
      "         财经       0.96      0.99      0.97      1000\n",
      "         房产       1.00      1.00      1.00      1000\n",
      "         家居       0.96      0.88      0.92      1000\n",
      "         教育       0.90      0.93      0.91      1000\n",
      "         科技       0.91      0.98      0.94      1000\n",
      "         时尚       0.95      0.97      0.96      1000\n",
      "         时政       0.95      0.92      0.93      1000\n",
      "         游戏       0.99      0.94      0.96      1000\n",
      "         娱乐       0.97      0.97      0.97      1000\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[992   0   0   1   2   3   0   2   0   0]\n",
      " [  0 990   0   0   1   2   0   7   0   0]\n",
      " [  0   0 996   1   2   1   0   0   0   0]\n",
      " [  3  14   1 882  18  35  14  26   2   5]\n",
      " [  1   7   2   8 925  23  16   8   2   8]\n",
      " [  0   0   0   3   4 982   5   1   3   2]\n",
      " [  2   0   0   6  10   3 969   0   1   9]\n",
      " [  0  17   0   5  35  20   1 917   1   4]\n",
      " [  2   5   1   4  18   9  14   0 940   7]\n",
      " [  1   0   0   6   9   7   4   1   2 970]]\n",
      "Time usage: 0:00:31\n"
     ]
    }
   ],
   "source": [
    "test() # compared with the original chinese one, it is improved by 1.51% 94.12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train in jupyter\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from data.cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab\n",
    "\n",
    "\n",
    "# 这个代码拆解了很多，最后才进行了训练\n",
    "base_dir = 'data/cnews'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    tensorboard_dir = 'tensorboard/textcnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            feed_dict[model.keep_prob] = config.dropout_keep_prob\n",
    "            session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(session, x_test, y_test)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果\n",
    "    for i in range(num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        feed_dict = {\n",
    "            model.input_x: x_test[start_id:end_id],\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CNN model...\n",
      "Tensor(\"embedding/embedding_lookup:0\", shape=(?, 600, 64), dtype=float32, device=/device:CPU:0)\n",
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "Time usage: 0:00:19\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Iter:      0, Train Loss:    2.5, Train Acc:  10.94%, Val Loss:    2.5, Val Acc:   9.38%, Time: 0:00:07 *\n",
      "Iter:    100, Train Loss:    1.6, Train Acc:  62.50%, Val Loss:    1.8, Val Acc:  49.92%, Time: 0:01:04 *\n",
      "Iter:    200, Train Loss:    0.7, Train Acc:  78.12%, Val Loss:   0.94, Val Acc:  74.62%, Time: 0:02:00 *\n",
      "Iter:    300, Train Loss:   0.27, Train Acc:  90.62%, Val Loss:   0.74, Val Acc:  78.36%, Time: 0:03:01 *\n",
      "Iter:    400, Train Loss:   0.25, Train Acc:  95.31%, Val Loss:   0.51, Val Acc:  85.66%, Time: 0:04:00 *\n",
      "Iter:    500, Train Loss:   0.18, Train Acc:  92.19%, Val Loss:   0.47, Val Acc:  86.90%, Time: 0:04:59 *\n",
      "Iter:    600, Train Loss:    0.2, Train Acc:  90.62%, Val Loss:   0.46, Val Acc:  87.28%, Time: 0:06:00 *\n",
      "Iter:    700, Train Loss:   0.32, Train Acc:  87.50%, Val Loss:   0.46, Val Acc:  86.16%, Time: 0:06:58 \n",
      "Epoch: 2\n",
      "Iter:    800, Train Loss:   0.25, Train Acc:  92.19%, Val Loss:   0.46, Val Acc:  86.42%, Time: 0:07:55 \n",
      "Iter:    900, Train Loss:   0.12, Train Acc:  95.31%, Val Loss:   0.36, Val Acc:  89.68%, Time: 0:08:49 *\n",
      "Iter:   1000, Train Loss:  0.073, Train Acc:  98.44%, Val Loss:   0.33, Val Acc:  91.20%, Time: 0:09:43 *\n",
      "Iter:   1100, Train Loss:   0.17, Train Acc:  93.75%, Val Loss:    0.3, Val Acc:  91.30%, Time: 0:10:36 *\n",
      "Iter:   1200, Train Loss:   0.23, Train Acc:  93.75%, Val Loss:   0.35, Val Acc:  90.16%, Time: 0:11:29 \n",
      "Iter:   1300, Train Loss:   0.15, Train Acc:  95.31%, Val Loss:   0.31, Val Acc:  92.24%, Time: 0:12:23 *\n",
      "Iter:   1400, Train Loss:   0.12, Train Acc:  95.31%, Val Loss:   0.34, Val Acc:  91.14%, Time: 0:13:18 \n",
      "Iter:   1500, Train Loss:   0.38, Train Acc:  87.50%, Val Loss:    0.3, Val Acc:  91.64%, Time: 0:14:13 \n",
      "Epoch: 3\n",
      "Iter:   1600, Train Loss:  0.047, Train Acc: 100.00%, Val Loss:   0.29, Val Acc:  92.46%, Time: 0:15:09 *\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-eb135de84175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-a8c8c1d71461>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 运行优化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mtotal_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main func\n",
    "tf.reset_default_graph() # 添加这句的目的是在jupyter notebook里面清楚缓存，不然会出现变量重用的问题\n",
    "print('Configuring CNN model...')\n",
    "config = TCNNConfig()\n",
    "if not os.path.exists(vocab_dir):  # 如果不存在词汇表，重建\n",
    "    build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "categories, cat_to_id = read_category()\n",
    "words, word_to_id = read_vocab(vocab_dir)\n",
    "config.vocab_size = len(words)\n",
    "model = TextCNN(config)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cnn_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-94646d5b3225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcnn_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTCNNConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnews_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_category\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cnn_model'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "from cnn_model import TCNNConfig, TextCNN\n",
    "from data.cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab\n",
    "\n",
    "\n",
    "# 这个代码拆解了很多，最后才进行了训练\n",
    "base_dir = 'data/cnews'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    tensorboard_dir = 'tensorboard/textcnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            feed_dict[model.keep_prob] = config.dropout_keep_prob\n",
    "            session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(session, x_test, y_test)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果\n",
    "    for i in range(num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        feed_dict = {\n",
    "            model.input_x: x_test[start_id:end_id],\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 2 or sys.argv[1] not in ['train', 'test']:\n",
    "        raise ValueError(\"\"\"usage: python run_cnn.py [train / test]\"\"\")\n",
    "\n",
    "    print('Configuring CNN model...')\n",
    "    config = TCNNConfig()\n",
    "    if not os.path.exists(vocab_dir):  # 如果不存在词汇表，重建\n",
    "        build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "    categories, cat_to_id = read_category()\n",
    "    words, word_to_id = read_vocab(vocab_dir)\n",
    "    config.vocab_size = len(words)\n",
    "    model = TextCNN(config)\n",
    "\n",
    "    if sys.argv[1] == 'train':\n",
    "        train()\n",
    "    else:\n",
    "        test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TesxtCNN(object):\n",
    "    \"\"\"CNN for text classification .\n",
    "    Use an embedding layer, followed by a convolutional layer, \n",
    "    max-pooling layer and a softmax layer\"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters):\n",
    "        # placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(dtype=tf.int32, shape=[None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(dtype=tf.float32, shape = [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(dtype=tf.float32, name=\"dropout_keep_prob\")\n",
    "    \n",
    "        # define the embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            # w is the embedding matrix thar we learn during training and use randomuniform distribution to initialize it\n",
    "            W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "            name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # define convlutional layer and maxpooling layer\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool\" % filter_size):\n",
    "                filter_shape=[filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.trucated_nomal(filter_shape, stddev=0.1), name = \"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape = [num_filters]), anme = \"b\")\n",
    "                conv = tf.nn.con2d(self.embedded_chars_expanded,\n",
    "                                  W,\n",
    "                                  strides = [1, 1, 1, 1], # all 1 in four channels\n",
    "                                  padding = \"VALID\", # means the dimension will change after conv\n",
    "                                  name = \"conv\")\n",
    "                # apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")\n",
    "\n",
    "                ##########above is conv\n",
    "                # mxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                strides = [1,1,1,1],\n",
    "                padding = \"VALID\",\n",
    "                name = \"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "    # Performing max-pooling over the output of a specific filter \n",
    "    # size leaves us with a tensor of shape \n",
    "    # [batch_size, 1, 1, num_filters]. \n",
    "    # [the size of batch(num of sentence in a batch),choose the max one , keep ,channels]\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs) # 最后一维进行拼接\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.trucated_normal([num_filters_total, num_classes], staddev = 0.1), name = \"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape = [num_classes]), name = \"b\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"score\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # cal mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # calculate accuaray\n",
    "        with tf.name_scope(\"accuaracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name = \"accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip tensorboard maybe later review it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FLAGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f9f8b0dd2d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     session_conf = tf.ConfigProto(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mallow_soft_placement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_soft_platment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     log_device_placement = FLAGS.log_device_platment)\n\u001b[1;32m      5\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FLAGS' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "    allow_soft_placement = FLAGS.allow_soft_platment,\n",
    "    log_device_placement = FLAGS.log_device_platment)\n",
    "    sess = tf.Session(config = session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "        sequence_length = x_train.shape[1],\n",
    "        num_classes = 2,\n",
    "        vocab_size = len(vocabulary),\n",
    "        embedding_size=FLAGS.embedding_dim,\n",
    "        filter_sizes = map(int, FLAGS.filter_sizes.split(\",\")),\n",
    "        num_filters = FLAGS.num_filters)\n",
    "        \n",
    "        # define optimize network's loss function using adam optimizer\n",
    "        \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        \n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-f9b9910bc510>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f9b9910bc510>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    checkpoint_dir = os.path.abspath(os.path.join(out_dir, &quot;checkpoints&quot;))\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Checkpointing\n",
    "checkpoint_dir = os.path.abspath(os.path.join(out_dir, &quot;checkpoints&quot;))\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, &quot;model&quot;)\n",
    "# Tensorflow assumes this directory already exists so we need to create it\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "saver = tf.train.Saver(tf.all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dc288af99664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "sess.run(tf.initialize_all_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "    }\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    train_summary_writer.add_summary(summaries, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_step(x_batch, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, summaries, loss, accuracy = sess.run(\n",
    "        [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
