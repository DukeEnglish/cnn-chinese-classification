{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN配置参数\"\"\"\n",
    "    embedding_size = 64  # 词向量维度\n",
    "    seq_length = 600  # 序列长度\n",
    "    num_classes = 10  # 类别数\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺寸\n",
    "    vocab_size = 5000  # 词汇表大小\n",
    "\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 10  # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 100  # 每多少轮输出一次结果\n",
    "    save_per_batch = 10  # 每多少轮存入tensorboard\n",
    "\n",
    "    l2_reg_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现思路：\n",
    "# 将对x的标签化放在模型中完成，只将对y的label化放在外面。个人倾向于都放在模型中，后续进行修改\n",
    "# embedding - cnn - maxpooling - fc - dropout - classify - result\n",
    "class TextCNN(object):\n",
    "    def __init__(self, config, embedding):\n",
    "        self.config = config\n",
    "        \n",
    "        # placrholders for three inputs\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # init cnn model different the original form, i prefer this one\n",
    "        self.cnn()\n",
    "    def cnn(self, ):\n",
    "        # embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            # w is the embedding matrix thar we learn during training and use randomuniform distribution to initialize it\n",
    "            # I use a pretrained vector to init the embedding weight\n",
    "            W = tf.Variable(tf.cast(embedding, dtype=tf.float32, name = 'word2vec'), name = 'W')\n",
    "            embedded_ouput = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            print('embedded_ouput', embedded_ouput)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        # define the cnn layer with max pooling\n",
    "        # use 2d input instead of 4d so we could jsut conv1d, simplify the calculation of dimentsions\n",
    "        with tf.name_scope('cnn'):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedded_ouput, self.config.num_filters, self.config.kernel_size, name='cnn_conv')\n",
    "            # global max pooling layer\n",
    "            # just get the max one on dimension 1\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        # add a fully-connect with a dropout\n",
    "        \n",
    "        with tf.name_scope(\"dropout\", ):\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "            \n",
    "        with tf.name_scope(\"score\", ):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[self.config.hidden_dim, self.config.num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.config.num_classes]), name=\"b\")\n",
    "            self.l2_loss += tf.nn.l2_loss(W)\n",
    "            self.l2_loss += tf.nn.l2_loss(b)\n",
    "            \n",
    "            self.logits = tf.nn.xw_plus_b(fc, W, b, name=\"scores\")\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别\n",
    "        \n",
    "        with tf.name_scope(\"loss\",):\n",
    "            # loss function cross_entropy\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy) + self.config.l2_reg_lambda * self.l2_loss\n",
    "            \n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\", ):\n",
    "            # 准确率\n",
    "            correct_predictions = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name = \"accuracy\")\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CNN model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got array([ 8.71970e-02, -8.34350e-02,  5.79560e-02,  1.43120e-01,\n       -6.80000e-05,  1.23272e-01,  2.24390e-02, -3.23317e-01,\n       -2.31756e-01, -9.26200e-03, -2.11264e-01, -2.16980e-02,\n        2.46367e-01,  1.79090e-01,  5.41170e-02,  7.76380e-02,\n       -5.45550e-02, -5.06300e-02,  7.23610e-02,  1.03788e-01,\n        2.40412e-01, -3.86661e-01,  9.60130e-02, -1.92248e-01,\n       -1.17727e-01,  2.29500e-03,  4.58870e-02,  3.01900e-03,\n        1.68361e-01,  9.55230e-02, -1.97418e-01,  1.15341e-01,\n        2.14799e-01, -2.12581e-01,  9.59420e-02, -6.13190e-02,\n        8.91710e-02, -8.79190e-02,  1.13341e-01,  6.62700e-03,\n       -5.92500e-03, -2.00036e-01, -1.63850e-02,  9.37650e-02,\n        3.74700e-02, -9.29840e-02, -2.07072e-01, -3.88760e-02,\n       -8.49360e-02, -2.07676e-01, -1.01227e-01, -1.27119e-01,\n       -5.60400e-02, -9.51780e-02,  1.58290e-01,  7.62910e-02,\n        2.62668e-01,  1.46720e-01,  1.41091e-01, -1.34479e-01,\n       -2.13100e-01, -5.11230e-02,  1.43333e-01, -9.53900e-03,\n       -4.88910e-02,  1.54130e-02,  9.26400e-03,  1.97350e-02,\n       -1.22203e-01, -2.62570e-02,  5.64420e-02,  1.26553e-01,\n        9.76150e-02,  3.88840e-02, -1.81495e-01,  1.75468e-01,\n       -4.12880e-02,  3.29550e-02,  6.53140e-02,  1.49118e-01,\n       -1.73668e-01,  1.53916e-01, -1.18243e-01,  3.89850e-02,\n       -9.44480e-02,  6.70180e-02, -1.04510e-01,  3.18760e-02,\n       -1.90216e-01, -1.35015e-01,  9.43600e-03, -5.22100e-03,\n        3.38130e-02,  5.82360e-02, -1.36879e-01, -4.01890e-02,\n        1.12950e-01,  4.51140e-02, -8.46500e-02,  1.11534e-01],\n      dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-c80d7e865f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Configuring CNN model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-d3e0424b4a24>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, embedding)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# init cnn model different the original form, i prefer this one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# embedding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-d3e0424b4a24>\u001b[0m in \u001b[0;36mcnn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# w is the embedding matrix thar we learn during training and use randomuniform distribution to initialize it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# I use a pretrained vector to init the embedding weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'W'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0membedded_ouput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embedded_ouput'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_ouput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m       \u001b[0;31m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;31m# strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    233\u001b[0m                                          as_ref=False):\n\u001b[1;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    212\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 214\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    531\u001b[0m     raise TypeError(\n\u001b[1;32m    532\u001b[0m         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\n\u001b[0;32m--> 533\u001b[0;31m   \u001b[0mappend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtensorflow/python/framework/fast_tensor_util.pyx\u001b[0m in \u001b[0;36mtensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xgb/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 68\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got array([ 8.71970e-02, -8.34350e-02,  5.79560e-02,  1.43120e-01,\n       -6.80000e-05,  1.23272e-01,  2.24390e-02, -3.23317e-01,\n       -2.31756e-01, -9.26200e-03, -2.11264e-01, -2.16980e-02,\n        2.46367e-01,  1.79090e-01,  5.41170e-02,  7.76380e-02,\n       -5.45550e-02, -5.06300e-02,  7.23610e-02,  1.03788e-01,\n        2.40412e-01, -3.86661e-01,  9.60130e-02, -1.92248e-01,\n       -1.17727e-01,  2.29500e-03,  4.58870e-02,  3.01900e-03,\n        1.68361e-01,  9.55230e-02, -1.97418e-01,  1.15341e-01,\n        2.14799e-01, -2.12581e-01,  9.59420e-02, -6.13190e-02,\n        8.91710e-02, -8.79190e-02,  1.13341e-01,  6.62700e-03,\n       -5.92500e-03, -2.00036e-01, -1.63850e-02,  9.37650e-02,\n        3.74700e-02, -9.29840e-02, -2.07072e-01, -3.88760e-02,\n       -8.49360e-02, -2.07676e-01, -1.01227e-01, -1.27119e-01,\n       -5.60400e-02, -9.51780e-02,  1.58290e-01,  7.62910e-02,\n        2.62668e-01,  1.46720e-01,  1.41091e-01, -1.34479e-01,\n       -2.13100e-01, -5.11230e-02,  1.43333e-01, -9.53900e-03,\n       -4.88910e-02,  1.54130e-02,  9.26400e-03,  1.97350e-02,\n       -1.22203e-01, -2.62570e-02,  5.64420e-02,  1.26553e-01,\n        9.76150e-02,  3.88840e-02, -1.81495e-01,  1.75468e-01,\n       -4.12880e-02,  3.29550e-02,  6.53140e-02,  1.49118e-01,\n       -1.73668e-01,  1.53916e-01, -1.18243e-01,  3.89850e-02,\n       -9.44480e-02,  6.70180e-02, -1.04510e-01,  3.18760e-02,\n       -1.90216e-01, -1.35015e-01,  9.43600e-03, -5.22100e-03,\n        3.38130e-02,  5.82360e-02, -1.36879e-01, -4.01890e-02,\n        1.12950e-01,  4.51140e-02, -8.46500e-02,  1.11534e-01],\n      dtype=float32)"
     ]
    }
   ],
   "source": [
    "# main func\n",
    "tf.reset_default_graph() # 添加这句的目的是在jupyter notebook里面clean缓存，不然会出现变量重用的问题\n",
    "print('Configuring CNN model...')\n",
    "\n",
    "model = TextCNN(config, embedding)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train in jupyter\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from data.cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab\n",
    "\n",
    "\n",
    "# 这个代码拆解了很多，最后才进行了训练\n",
    "base_dir = 'data/cnews'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob,\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    tensorboard_dir = 'tensorboard/textcnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                print('write reasult to tensorboard scaler')\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "#                     sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "#                 s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            feed_dict[model.keep_prob] = config.dropout_keep_prob\n",
    "            session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "\n",
    "    session = tf.Session()\n",
    "    \n",
    "\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(session, x_test, y_test)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果\n",
    "    for i in range(num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        feed_dict = {\n",
    "            model.input_x: x_test[start_id:end_id],\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "def loadWord2Vec(wv_filename, words):\n",
    "    wordVec = models.KeyedVectors.load_word2vec_format(wv_filename, binary=False)\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    cnt = 0\n",
    "    tmp = []\n",
    "    \n",
    "    # lookup the embedding\n",
    "    for w in words:\n",
    "        try:\n",
    "            vocab.append(w)\n",
    "            embd.append(wordVec.wv[w])   \n",
    "        except Exception as e:\n",
    "            print('error',e)\n",
    "            print(w + \"is not in pretained vector\")\n",
    "            tmp.append(w)\n",
    "    \n",
    "            \n",
    "    # 获取wordvector的纬度，添加unk并赋值指定维度的0\n",
    "    word_dim = len(embd[-1])\n",
    "    vocab.append(\"UNK\")\n",
    "    embd.append(np.random.randn(word_dim))   \n",
    "    vocab.append(\"<PAD>\")\n",
    "    embd.append(np.zeros(word_dim)) \n",
    "    for w in tmp:\n",
    "        vocab.append(w)\n",
    "        embd.append((np.random.randn(word_dim)))\n",
    "    \n",
    "    print (\"loaded word2vec\")\n",
    "    print('total vocab', len(vocab))\n",
    "    print(len(embd), 'word vectors found')\n",
    "    return vocab,embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD>'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error \"word '<PAD>' not in vocabulary\"\n",
      "<PAD>is not in pretained vector\n",
      "error \"word '，' not in vocabulary\"\n",
      "，is not in pretained vector\n",
      "error \"word '' not in vocabulary\"\n",
      "is not in pretained vector\n",
      "error \"word '：' not in vocabulary\"\n",
      "：is not in pretained vector\n",
      "error \"word '' not in vocabulary\"\n",
      "is not in pretained vector\n",
      "error \"word '？' not in vocabulary\"\n",
      "？is not in pretained vector\n",
      "error \"word '；' not in vocabulary\"\n",
      "；is not in pretained vector\n",
      "error \"word '—' not in vocabulary\"\n",
      "—is not in pretained vector\n",
      "error \"word '）' not in vocabulary\"\n",
      "）is not in pretained vector\n",
      "error \"word '（' not in vocabulary\"\n",
      "（is not in pretained vector\n",
      "error \"word '' not in vocabulary\"\n",
      "is not in pretained vector\n",
      "error \"word '』' not in vocabulary\"\n",
      "』is not in pretained vector\n",
      "error \"word '『' not in vocabulary\"\n",
      "『is not in pretained vector\n",
      "error \"word '後' not in vocabulary\"\n",
      "後is not in pretained vector\n",
      "error \"word '摺' not in vocabulary\"\n",
      "摺is not in pretained vector\n",
      "error \"word '」' not in vocabulary\"\n",
      "」is not in pretained vector\n",
      "error \"word '「' not in vocabulary\"\n",
      "「is not in pretained vector\n",
      "error \"word '捱' not in vocabulary\"\n",
      "捱is not in pretained vector\n",
      "error \"word '┊' not in vocabulary\"\n",
      "┊is not in pretained vector\n",
      "loaded word2vec\n",
      "total vocab 5021\n",
      "5002 word vectors found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljy/miniconda2/envs/xgb/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "config = TCNNConfig()\n",
    "if not os.path.exists(vocab_dir):  # 如果不存在词汇表，重建\n",
    "    build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "categories, cat_to_id = read_category()\n",
    "words, word_to_id = read_vocab(vocab_dir)\n",
    "\n",
    "# read wordvec from pretrained one\n",
    "filename = \"wiki_100.utf8.txt\"\n",
    "vocab,embd = loadWord2Vec(filename, words)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)\n",
    "# \n",
    "config.vocab_size = len(vocab) # len(words)\n",
    "config.embedding_size = len(embd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CNN model...\n",
      "embedded_ouput Tensor(\"embedding/embedding_lookup:0\", shape=(?, 600, 100), dtype=float32, device=/device:CPU:0)\n",
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "Time usage: 0:00:13\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "write reasult to tensorboard scaler\n",
      "Iter:      0, Train Loss:    3.3, Train Acc:  12.50%, Val Loss:    3.3, Val Acc:   8.76%, Time: 0:00:11 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    100, Train Loss:    1.6, Train Acc:  68.75%, Val Loss:    1.7, Val Acc:  64.24%, Time: 0:01:22 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    200, Train Loss:   0.75, Train Acc:  89.06%, Val Loss:    1.0, Val Acc:  75.78%, Time: 0:02:35 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    300, Train Loss:   0.42, Train Acc:  93.75%, Val Loss:   0.77, Val Acc:  80.72%, Time: 0:03:44 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    400, Train Loss:   0.36, Train Acc:  92.19%, Val Loss:   0.66, Val Acc:  83.64%, Time: 0:04:53 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    500, Train Loss:   0.33, Train Acc:  92.19%, Val Loss:    0.6, Val Acc:  84.96%, Time: 0:06:01 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    600, Train Loss:   0.31, Train Acc:  93.75%, Val Loss:   0.52, Val Acc:  87.52%, Time: 0:07:10 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    700, Train Loss:   0.15, Train Acc: 100.00%, Val Loss:   0.47, Val Acc:  89.12%, Time: 0:08:19 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Epoch: 2\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    800, Train Loss:   0.17, Train Acc:  98.44%, Val Loss:   0.46, Val Acc:  88.62%, Time: 0:09:28 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    900, Train Loss:   0.35, Train Acc:  96.88%, Val Loss:    0.4, Val Acc:  91.16%, Time: 0:10:37 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1000, Train Loss:   0.19, Train Acc:  95.31%, Val Loss:   0.41, Val Acc:  90.30%, Time: 0:11:47 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1100, Train Loss:   0.26, Train Acc:  95.31%, Val Loss:   0.37, Val Acc:  91.52%, Time: 0:12:56 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1200, Train Loss:   0.23, Train Acc:  95.31%, Val Loss:   0.37, Val Acc:  91.50%, Time: 0:14:05 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1300, Train Loss:  0.093, Train Acc: 100.00%, Val Loss:   0.36, Val Acc:  91.16%, Time: 0:15:14 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1400, Train Loss:   0.15, Train Acc:  98.44%, Val Loss:   0.33, Val Acc:  92.80%, Time: 0:16:23 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1500, Train Loss:   0.12, Train Acc:  98.44%, Val Loss:   0.33, Val Acc:  91.80%, Time: 0:17:32 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Epoch: 3\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1600, Train Loss:   0.18, Train Acc:  96.88%, Val Loss:    0.3, Val Acc:  92.52%, Time: 0:18:40 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write reasult to tensorboard scaler\n",
      "Iter:   1700, Train Loss:   0.16, Train Acc:  96.88%, Val Loss:   0.32, Val Acc:  91.88%, Time: 0:19:49 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1800, Train Loss:   0.24, Train Acc:  93.75%, Val Loss:    0.3, Val Acc:  92.44%, Time: 0:20:57 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1900, Train Loss:   0.16, Train Acc:  95.31%, Val Loss:   0.31, Val Acc:  91.88%, Time: 0:22:07 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2000, Train Loss:   0.15, Train Acc:  96.88%, Val Loss:   0.31, Val Acc:  91.62%, Time: 0:23:16 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2100, Train Loss:   0.13, Train Acc:  95.31%, Val Loss:   0.29, Val Acc:  92.52%, Time: 0:24:24 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2200, Train Loss:   0.26, Train Acc:  93.75%, Val Loss:   0.28, Val Acc:  93.10%, Time: 0:25:33 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2300, Train Loss:   0.16, Train Acc:  96.88%, Val Loss:   0.29, Val Acc:  92.46%, Time: 0:26:42 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Epoch: 4\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2400, Train Loss:   0.14, Train Acc:  95.31%, Val Loss:   0.28, Val Acc:  93.10%, Time: 0:27:53 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2500, Train Loss:  0.066, Train Acc: 100.00%, Val Loss:   0.26, Val Acc:  93.78%, Time: 0:29:02 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2600, Train Loss:   0.13, Train Acc:  96.88%, Val Loss:   0.28, Val Acc:  92.86%, Time: 0:30:12 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2700, Train Loss:   0.23, Train Acc:  92.19%, Val Loss:   0.26, Val Acc:  93.54%, Time: 0:31:34 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2800, Train Loss:   0.14, Train Acc:  95.31%, Val Loss:   0.28, Val Acc:  92.92%, Time: 0:32:55 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2900, Train Loss:   0.16, Train Acc:  95.31%, Val Loss:   0.25, Val Acc:  93.62%, Time: 0:34:14 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3000, Train Loss:  0.069, Train Acc: 100.00%, Val Loss:   0.27, Val Acc:  92.56%, Time: 0:35:27 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3100, Train Loss:  0.076, Train Acc:  98.44%, Val Loss:   0.27, Val Acc:  93.08%, Time: 0:36:39 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Epoch: 5\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3200, Train Loss:  0.087, Train Acc:  98.44%, Val Loss:   0.23, Val Acc:  94.48%, Time: 0:37:51 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3300, Train Loss:  0.044, Train Acc: 100.00%, Val Loss:   0.28, Val Acc:  92.26%, Time: 0:39:09 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3400, Train Loss:  0.072, Train Acc: 100.00%, Val Loss:   0.26, Val Acc:  93.14%, Time: 0:40:25 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3500, Train Loss:  0.054, Train Acc: 100.00%, Val Loss:   0.25, Val Acc:  93.60%, Time: 0:41:36 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3600, Train Loss:   0.06, Train Acc: 100.00%, Val Loss:   0.25, Val Acc:  93.68%, Time: 0:42:50 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3700, Train Loss:  0.039, Train Acc: 100.00%, Val Loss:   0.26, Val Acc:  93.40%, Time: 0:44:13 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3800, Train Loss:  0.058, Train Acc: 100.00%, Val Loss:   0.26, Val Acc:  93.30%, Time: 0:45:27 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   3900, Train Loss:  0.062, Train Acc: 100.00%, Val Loss:   0.26, Val Acc:  93.50%, Time: 0:46:44 \n",
      "Epoch: 6\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   4000, Train Loss:  0.072, Train Acc: 100.00%, Val Loss:   0.23, Val Acc:  94.16%, Time: 1:27:12 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   4100, Train Loss:  0.049, Train Acc: 100.00%, Val Loss:   0.23, Val Acc:  93.96%, Time: 1:29:22 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   4200, Train Loss:  0.075, Train Acc:  98.44%, Val Loss:   0.23, Val Acc:  94.20%, Time: 2:24:49 \n",
      "No optimization for a long time, auto-stopping...\n"
     ]
    }
   ],
   "source": [
    "# main func\n",
    "tf.reset_default_graph() # 添加这句的目的是在jupyter notebook里面clean缓存，不然会出现变量重用的问题\n",
    "print('Configuring CNN model...')\n",
    "\n",
    "model = TextCNN(config, embedding)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/textcnn/best_validation\n",
      "Testing...\n",
      "Test Loss:   0.16, Test Acc:  96.20%\n",
      "Precision, Recall and F1-Score...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         体育       1.00      0.99      0.99      1000\n",
      "         财经       0.97      0.99      0.98      1000\n",
      "         房产       1.00      1.00      1.00      1000\n",
      "         家居       0.98      0.88      0.92      1000\n",
      "         教育       0.87      0.96      0.91      1000\n",
      "         科技       0.92      0.99      0.95      1000\n",
      "         时尚       0.97      0.96      0.97      1000\n",
      "         时政       0.96      0.92      0.94      1000\n",
      "         游戏       0.99      0.96      0.98      1000\n",
      "         娱乐       0.98      0.97      0.97      1000\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[992   0   0   0   4   1   0   3   0   0]\n",
      " [  0 992   1   0   2   1   0   4   0   0]\n",
      " [  0   0 996   0   2   2   0   0   0   0]\n",
      " [  1  13   0 875  42  27  10  25   1   6]\n",
      " [  0   3   0   4 956  19   4   7   2   5]\n",
      " [  0   0   0   2   7 988   1   0   2   0]\n",
      " [  1   0   0   4  13   5 965   0   3   9]\n",
      " [  0  15   0   2  48  12   0 921   1   1]\n",
      " [  0   2   1   1  13  10   7   0 964   2]\n",
      " [  1   0   0   5   9   7   4   1   2 971]]\n",
      "Time usage: 0:00:45\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
