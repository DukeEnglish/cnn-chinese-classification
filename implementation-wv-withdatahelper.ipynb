{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import keras as kr\n",
    "from gensim import models\n",
    "\n",
    "class Datahelper(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filename):\n",
    "        contents, labels = [], []\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    label, content = line.strip().split('\\t')\n",
    "                    if content:\n",
    "                        contents.append(list((content)))\n",
    "                        labels.append((label))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "        return contents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data helper\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import keras as kr\n",
    "from gensim import models\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list((content)))\n",
    "                    labels.append((label))\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n",
    "\n",
    "\n",
    "def build_vocab(train_dir, vocab_dir, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train, _ = read_file(train_dir)\n",
    "\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open_file(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')\n",
    "\n",
    "\n",
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    # words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "    with open_file(vocab_dir) as fp:\n",
    "        # 如果是py2 则每个值都转化为unicode\n",
    "        words = [(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    # word: id 把所有的汉字都id化了\n",
    "    return words, word_to_id\n",
    "\n",
    "\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "\n",
    "    categories = [(x) for x in categories]\n",
    "\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "    return categories, cat_to_id\n",
    "\n",
    "\n",
    "def to_words(content, words):\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "    return ''.join(words[x] for x in content)\n",
    "\n",
    "\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    \"\"\"将文件转换为id表示\"\"\"\n",
    "    contents, labels = read_file(filename)\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "\n",
    "    return x_pad, y_pad\n",
    "\n",
    "def loadWord2Vec(wv_filename, words):\n",
    "    wordVec = models.KeyedVectors.load_word2vec_format(wv_filename, binary=False)\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    cnt = 0\n",
    "    tmp = []\n",
    "    \n",
    "    # lookup the embedding\n",
    "    for w in words:\n",
    "        try:\n",
    "            vocab.append(w)\n",
    "            embd.append(wordVec.wv[w])   \n",
    "        except Exception as e:\n",
    "            print('error',e)\n",
    "            print(w + \"is not in pretained vector\")\n",
    "            tmp.append(w)\n",
    "    \n",
    "            \n",
    "    # 获取wordvector的纬度，添加unk并赋值指定维度的0\n",
    "    word_dim = len(embd[-1])\n",
    "    vocab.append(\"UNK\")\n",
    "    embd.append(np.random.randn(word_dim))   \n",
    "    vocab.append(\"<PAD>\")\n",
    "    embd.append(np.zeros(word_dim)) \n",
    "    for w in tmp:\n",
    "        vocab.append(w)\n",
    "        embd.append((np.random.randn(word_dim)))\n",
    "    \n",
    "    print (\"loaded word2vec\")\n",
    "    print('total vocab', len(vocab))\n",
    "    print(len(embd), 'word vectors found')\n",
    "    return vocab,embd\n",
    "\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcnnconfig\n",
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN配置参数\"\"\"\n",
    "    embedding_size = 64  # 词向量维度\n",
    "    seq_length = 600  # 序列长度\n",
    "    num_classes = 10  # 类别数\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺寸\n",
    "    vocab_size = 5000  # 词汇表大小\n",
    "\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 10  # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 100  # 每多少轮输出一次结果\n",
    "    save_per_batch = 10  # 每多少轮存入tensorboard\n",
    "\n",
    "    l2_reg_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textcnn model define\n",
    "# 实现思路：\n",
    "# 将对x的标签化放在模型中完成，只将对y的label化放在外面。个人倾向于都放在模型中，后续进行修改\n",
    "# embedding - cnn - maxpooling - fc - dropout - classify - result\n",
    "class TextCNN(object):\n",
    "    def __init__(self, config, embedding):\n",
    "        self.config = config\n",
    "        \n",
    "        # placrholders for three inputs\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # init cnn model different the original form, i prefer this one\n",
    "        self.cnn()\n",
    "    def cnn(self, ):\n",
    "        # embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            # w is the embedding matrix thar we learn during training and use randomuniform distribution to initialize it\n",
    "            # I use a pretrained vector to init the embedding weight\n",
    "            W = tf.Variable(tf.cast(embedding, dtype=tf.float32, name = 'word2vec'), name = 'W')\n",
    "            embedded_ouput = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            print('embedded_ouput', embedded_ouput)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        # define the cnn layer with max pooling\n",
    "        # use 2d input instead of 4d so we could jsut conv1d, simplify the calculation of dimentsions\n",
    "        with tf.name_scope('cnn'):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedded_ouput, self.config.num_filters, self.config.kernel_size, name='cnn_conv')\n",
    "            # global max pooling layer\n",
    "            # just get the max one on dimension 1\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        # add a fully-connect with a dropout\n",
    "        \n",
    "        with tf.name_scope(\"dropout\", ):\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "            \n",
    "        with tf.name_scope(\"score\", ):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[self.config.hidden_dim, self.config.num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.config.num_classes]), name=\"b\")\n",
    "            self.l2_loss += tf.nn.l2_loss(W)\n",
    "            self.l2_loss += tf.nn.l2_loss(b)\n",
    "            \n",
    "            self.logits = tf.nn.xw_plus_b(fc, W, b, name=\"scores\")\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别\n",
    "        \n",
    "        with tf.name_scope(\"loss\",):\n",
    "            # loss function cross_entropy\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy) + self.config.l2_reg_lambda * self.l2_loss\n",
    "            \n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\", ):\n",
    "            # 准确率\n",
    "            correct_predictions = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name = \"accuracy\")\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train in jupyter\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from data.cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab\n",
    "\n",
    "\n",
    "# 这个代码拆解了很多，最后才进行了训练\n",
    "base_dir = 'data/subcnews'\n",
    "train_dir = os.path.join(base_dir, 'subcnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'subcnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'subcnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'subcnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob,\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    tensorboard_dir = 'tensorboard/textcnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                print('write reasult to tensorboard scaler')\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "#                     sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "#                 s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            feed_dict[model.keep_prob] = config.dropout_keep_prob\n",
    "            session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "\n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "\n",
    "    session = tf.Session()\n",
    "    \n",
    "\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(session, x_test, y_test)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果\n",
    "    for i in range(num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        feed_dict = {\n",
    "            model.input_x: x_test[start_id:end_id],\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD>'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error \"word '<PAD>' not in vocabulary\"\n",
      "<PAD>is not in pretained vector\n",
      "error \"word '，' not in vocabulary\"\n",
      "，is not in pretained vector\n",
      "error \"word '' not in vocabulary\"\n",
      "is not in pretained vector\n",
      "error \"word '：' not in vocabulary\"\n",
      "：is not in pretained vector\n",
      "error \"word '' not in vocabulary\"\n",
      "is not in pretained vector\n",
      "error \"word '？' not in vocabulary\"\n",
      "？is not in pretained vector\n",
      "error \"word '；' not in vocabulary\"\n",
      "；is not in pretained vector\n",
      "error \"word '—' not in vocabulary\"\n",
      "—is not in pretained vector\n",
      "error \"word '）' not in vocabulary\"\n",
      "）is not in pretained vector\n",
      "error \"word '（' not in vocabulary\"\n",
      "（is not in pretained vector\n",
      "error \"word '' not in vocabulary\"\n",
      "is not in pretained vector\n",
      "error \"word '』' not in vocabulary\"\n",
      "』is not in pretained vector\n",
      "error \"word '『' not in vocabulary\"\n",
      "『is not in pretained vector\n",
      "error \"word '後' not in vocabulary\"\n",
      "後is not in pretained vector\n",
      "error \"word '摺' not in vocabulary\"\n",
      "摺is not in pretained vector\n",
      "error \"word '」' not in vocabulary\"\n",
      "」is not in pretained vector\n",
      "error \"word '「' not in vocabulary\"\n",
      "「is not in pretained vector\n",
      "error \"word '捱' not in vocabulary\"\n",
      "捱is not in pretained vector\n",
      "error \"word '┊' not in vocabulary\"\n",
      "┊is not in pretained vector\n",
      "loaded word2vec\n",
      "total vocab 5021\n",
      "5002 word vectors found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljy/miniconda2/envs/xgb/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "config = TCNNConfig()\n",
    "if not os.path.exists(vocab_dir):  # 如果不存在词汇表，重建\n",
    "    build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "categories, cat_to_id = read_category()\n",
    "words, word_to_id = read_vocab(vocab_dir)\n",
    "\n",
    "# read wordvec from pretrained one\n",
    "filename = \"wiki_100.utf8.txt\"\n",
    "vocab,embd = loadWord2Vec(filename, words)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)\n",
    "# \n",
    "config.vocab_size = len(vocab) # len(words)\n",
    "config.embedding_size = len(embd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CNN model...\n",
      "embedded_ouput Tensor(\"embedding/embedding_lookup:0\", shape=(?, 600, 100), dtype=float32, device=/device:CPU:0)\n",
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "Time usage: 0:00:13\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "write reasult to tensorboard scaler\n",
      "Iter:      0, Train Loss:    3.3, Train Acc:  12.50%, Val Loss:    3.3, Val Acc:   8.76%, Time: 0:00:11 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    100, Train Loss:    1.6, Train Acc:  68.75%, Val Loss:    1.7, Val Acc:  64.24%, Time: 0:01:22 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    200, Train Loss:   0.75, Train Acc:  89.06%, Val Loss:    1.0, Val Acc:  75.78%, Time: 0:02:35 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    300, Train Loss:   0.42, Train Acc:  93.75%, Val Loss:   0.77, Val Acc:  80.72%, Time: 0:03:44 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    400, Train Loss:   0.36, Train Acc:  92.19%, Val Loss:   0.66, Val Acc:  83.64%, Time: 0:04:53 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    500, Train Loss:   0.33, Train Acc:  92.19%, Val Loss:    0.6, Val Acc:  84.96%, Time: 0:06:01 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    600, Train Loss:   0.31, Train Acc:  93.75%, Val Loss:   0.52, Val Acc:  87.52%, Time: 0:07:10 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    700, Train Loss:   0.15, Train Acc: 100.00%, Val Loss:   0.47, Val Acc:  89.12%, Time: 0:08:19 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Epoch: 2\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    800, Train Loss:   0.17, Train Acc:  98.44%, Val Loss:   0.46, Val Acc:  88.62%, Time: 0:09:28 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:    900, Train Loss:   0.35, Train Acc:  96.88%, Val Loss:    0.4, Val Acc:  91.16%, Time: 0:10:37 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1000, Train Loss:   0.19, Train Acc:  95.31%, Val Loss:   0.41, Val Acc:  90.30%, Time: 0:11:47 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1100, Train Loss:   0.26, Train Acc:  95.31%, Val Loss:   0.37, Val Acc:  91.52%, Time: 0:12:56 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1200, Train Loss:   0.23, Train Acc:  95.31%, Val Loss:   0.37, Val Acc:  91.50%, Time: 0:14:05 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1300, Train Loss:  0.093, Train Acc: 100.00%, Val Loss:   0.36, Val Acc:  91.16%, Time: 0:15:14 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1400, Train Loss:   0.15, Train Acc:  98.44%, Val Loss:   0.33, Val Acc:  92.80%, Time: 0:16:23 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1500, Train Loss:   0.12, Train Acc:  98.44%, Val Loss:   0.33, Val Acc:  91.80%, Time: 0:17:32 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Epoch: 3\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1600, Train Loss:   0.18, Train Acc:  96.88%, Val Loss:    0.3, Val Acc:  92.52%, Time: 0:18:40 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write reasult to tensorboard scaler\n",
      "Iter:   1700, Train Loss:   0.16, Train Acc:  96.88%, Val Loss:   0.32, Val Acc:  91.88%, Time: 0:19:49 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1800, Train Loss:   0.24, Train Acc:  93.75%, Val Loss:    0.3, Val Acc:  92.44%, Time: 0:20:57 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   1900, Train Loss:   0.16, Train Acc:  95.31%, Val Loss:   0.31, Val Acc:  91.88%, Time: 0:22:07 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2000, Train Loss:   0.15, Train Acc:  96.88%, Val Loss:   0.31, Val Acc:  91.62%, Time: 0:23:16 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2100, Train Loss:   0.13, Train Acc:  95.31%, Val Loss:   0.29, Val Acc:  92.52%, Time: 0:24:24 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2200, Train Loss:   0.26, Train Acc:  93.75%, Val Loss:   0.28, Val Acc:  93.10%, Time: 0:25:33 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2300, Train Loss:   0.16, Train Acc:  96.88%, Val Loss:   0.29, Val Acc:  92.46%, Time: 0:26:42 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Epoch: 4\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2400, Train Loss:   0.14, Train Acc:  95.31%, Val Loss:   0.28, Val Acc:  93.10%, Time: 0:27:53 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2500, Train Loss:  0.066, Train Acc: 100.00%, Val Loss:   0.26, Val Acc:  93.78%, Time: 0:29:02 *\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n",
      "Iter:   2600, Train Loss:   0.13, Train Acc:  96.88%, Val Loss:   0.28, Val Acc:  92.86%, Time: 0:30:12 \n",
      "write reasult to tensorboard scaler\n",
      "write reasult to tensorboard scaler\n"
     ]
    }
   ],
   "source": [
    "# main func\n",
    "tf.reset_default_graph() # 添加这句的目的是在jupyter notebook里面clean缓存，不然会出现变量重用的问题\n",
    "print('Configuring CNN model...')\n",
    "\n",
    "model = TextCNN(config, embedding)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
